{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    converting the mobile rec csv file to parquet\n",
    "'''\n",
    "\n",
    "# the following code chunks the csv and then appends the chunks to convert them into parquet\n",
    "# it reduces the size from 4.4 GB to 1.88 GB\n",
    "'''\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Specify the path to your large CSV file\n",
    "csv_file     = '../data/review/mobilerec_final.csv'\n",
    "\n",
    "# Specify the path to the output Parquet file\n",
    "parquet_file = '../data/review/mobilerec_final.parquet'\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 100000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize the Parquet writer\n",
    "csv_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "\n",
    "# Iterate over chunks and write to Parquet\n",
    "for i, chunk in enumerate(csv_iterator):\n",
    "    # Convert the DataFrame to an Arrow Table\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "    # For the first chunk, create a new Parquet file\n",
    "    if i == 0:\n",
    "        pqwriter = pq.ParquetWriter(parquet_file, table.schema)\n",
    "    \n",
    "    # Append subsequent chunks to the Parquet file\n",
    "    pqwriter.write_table(table)\n",
    "\n",
    "# Close the Parquet writer\n",
    "pqwriter.close()\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LLM set-up</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "geminiModel = genai.GenerativeModel('gemini-1.5-pro')\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting gemini for extracting information from the games\n",
    "# for conversation, we have some of the information such as ads, security, permissions\n",
    "\n",
    "class LLM():\n",
    "    def __init__(self, geminiModel):\n",
    "        self.geminiModel = geminiModel\n",
    "\n",
    "    def geminiResponse(self,prompt,responseType):\n",
    "        response = self.geminiModel.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config = genai.types.GenerationConfig(\n",
    "                                candidate_count = 1,\n",
    "                                top_p=0.5,\n",
    "                                top_k=1,\n",
    "                                temperature=0\n",
    "                            )    \n",
    "                        )\n",
    "        \n",
    "        return(response.text)\n",
    "\n",
    "    def claudeResponse(self,prompt):\n",
    "        api_url = \"https://api.anthropic.com/v1/messages\"\n",
    "    \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-API-Key\": os.getenv(\"CLAUDE_API_KEY\"),\n",
    "        }\n",
    "    \n",
    "        data = {\n",
    "            \"model\": \"claude-3-sonnet-20240229\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    \n",
    "        response = requests.post(api_url, headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['content'][0]['text']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "    def OpenAIResponse(self,sys_prompt,user_prompt):\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        # Extract the response text\n",
    "        response_text = response.choices[0].message.content\n",
    "        return (response_text)\n",
    "    \n",
    "LLMModel = LLM(geminiModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Collecting information about games</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we collect all the information required from the description (marked by - -)\n",
    "'''\n",
    "(i) User Interface Design: from reviews\n",
    "(ii) Navigation: from reviews\n",
    "(iii) Accessibility: for whom the game is for\n",
    "(iv) Customization: from review\n",
    "(v) Functionality: from games - - -\n",
    "(vi) Performance: from reviews (ratings from games)\n",
    "(vii) Responsiveness: users\n",
    "(viii) Security: from apps games data\n",
    "(ix) Privacy: from apps games data \n",
    "(x) Permissions: from apps games data \n",
    "(xi) Data Collection: from apps games data \n",
    "(xii) Data Sharing: from apps games data \n",
    "(xiii) Updates; \n",
    "(xiv) Customer support;\n",
    "(xv) Reviews and ratings: from apps games data \n",
    "(xvi) Developer: from apps games data \n",
    "(xvii) Price: from apps games data \n",
    "(xviii) In-app purchases: from apps games data  \n",
    "(xix) Advertisement Frequency: from apps games data  or reviews\n",
    "(xx) Battery Drainage: from reviews\n",
    "(xxi) Summary of app:  - - -\n",
    "(xxii) who would you recommend this type to:  - - -\n",
    "(xxiii) adjectives for the app  - - -\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeminiPrompts(name,descript):\n",
    "    prompt = f\"\"\"You are given an app store optimization specialist, with an expertise in optimizing app metadata, including descriptions, to improve app visibility and downloads.\n",
    "    You are given the following name and the description of the app below.\n",
    "\n",
    "    ------------------------\n",
    "    Name of the app: {name}\n",
    "    Description of the app: {descript}\n",
    "\n",
    "    based on the description of the app, follow the step-by-step guide below:\n",
    "    1-summarize what the mobile app is about within 250 words. sumamry should be informative and DO NOT ANY EXTERNAL INFORMATION\n",
    "    2-decription may have features inforamtion. identify these main features of the app. ONLY if featuers are not givem, identify from the summary you created.\n",
    "    3-find adjectives for the app which should attract users\n",
    "    4-find 4-5 search keywords as who is the target audience for whom is the app designed for.\n",
    "\n",
    "    Output in the following text format. Strictly follow the format. NO special characters.\n",
    "\n",
    "    summary: summary of the app withinn 250 words\n",
    "    features: main features of the app\n",
    "    adjectives:best 4-5 adjectives for the app\n",
    "    search_term:keywords of 4-5 target audience for whom is the app designed for/words that would atttract users to this app\n",
    "    \"\"\"\n",
    "    return(prompt)\n",
    "\n",
    "def OpenAIPrompts(name,descript):\n",
    "    sys_prompt = f\"\"\"You are given an app store optimization specialist, with an expertise in optimizing app metadata, including descriptions, to improve app visibility and downloads.\n",
    "    Based on the description of the app, follow the step-by-step guide below:\n",
    "    1-summarize what the mobile app is about within 250 words. sumamry should be informative and DO NOT ANY EXTERNAL INFORMATION\n",
    "    2-decription may have features inforamtion. identify these main features of the app. ONLY if featuers are not givem, identify from the summary you created.\n",
    "    3-find adjectives for the app which should attract users\n",
    "    4-find 4-5 search keywords as who is the target audience for whom is the app designed for.\n",
    "\n",
    "    Output in the following text format. Strictly follow the format. NO special characters.\n",
    "\n",
    "    summary: summary of the app withinn 250 words\n",
    "    features: main features of the app\n",
    "    adjectives:best 4-5 adjectives for the app\n",
    "    search_term:keywords of 4-5 target audience for whom is the app designed for/words that would atttract users to this app\n",
    "\n",
    "    Think carefully if you have all these fields or not, if not, make sure you dont miss any of summary, feature, adjectives or search_term\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    You are given the following name and the description of the app below.\n",
    "    ------------------------\n",
    "    Name of the app: {name}\n",
    "    Description of the app: {descript}\n",
    "    \"\"\"\n",
    "\n",
    "    return(sys_prompt,user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10173 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723574400.936470 2435845 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      " 11%|█         | 1132/10173 [00:10<01:26, 104.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1134/10173 [00:26<04:28, 33.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1135/10173 [00:35<06:51, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1136/10173 [00:40<08:48, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1137/10173 [00:46<11:39, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1138/10173 [00:50<15:12,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1139/10173 [00:55<20:34,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1139/10173 [00:59<07:54, 19.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m GeminiPrompts(name,descript)\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mLLMModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeminiResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m information_dict[app_package] \u001b[38;5;241m=\u001b[39m response\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mLLM.geminiResponse\u001b[0;34m(self, prompt, responseType)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeminiResponse\u001b[39m(\u001b[38;5;28mself\u001b[39m,prompt,responseType):\n\u001b[0;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeminiModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcandidate_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:827\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mobilerec/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     response \u001b[38;5;241m=\u001b[39m LLMModel\u001b[38;5;241m.\u001b[39mOpenAIResponse(sys_prompt,user_prompt)\n\u001b[1;32m     38\u001b[0m     information_dict[app_package] \u001b[38;5;241m=\u001b[39m response\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     count_o \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/review/app_information_dict.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('../data/review/app_information_dict.json') as f:\n",
    "        information_dict = json.load(f)\n",
    "\n",
    "except:\n",
    "    # contains the text\n",
    "    information_dict = {}\n",
    "\n",
    "    with open('../data/review/app_information_dict.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(information_dict, f, indent=4)\n",
    "\n",
    "# after reading the dict, check if it contains the app_package and store into from LLMs (in plain text now)\n",
    "app_meta = pd.read_parquet(\"../data/review/app_meta.parquet\")\n",
    "count_g,count_o = 0,0\n",
    "\n",
    "for r in tqdm(range(len(app_meta))):\n",
    "    if r > 2000:\n",
    "        break\n",
    "\n",
    "    app_package = app_meta['app_package'].iloc[r]\n",
    "\n",
    "    if app_package not in information_dict.keys():\n",
    "        name     = app_meta['app_name'].iloc[r]\n",
    "        descript = app_meta['description'].iloc[r]\n",
    "\n",
    "        # first try gemini, if there is an error then go for chat gpt\n",
    "\n",
    "        try:\n",
    "            prompt = GeminiPrompts(name,descript)\n",
    "            response = LLMModel.geminiResponse(prompt,None)\n",
    "            information_dict[app_package] = response\n",
    "            time.sleep(3)\n",
    "            count_g += 1\n",
    "\n",
    "        except:\n",
    "            sys_prompt,user_prompt = OpenAIPrompts(name,descript)\n",
    "            response = LLMModel.OpenAIResponse(sys_prompt,user_prompt)\n",
    "            information_dict[app_package] = response\n",
    "            time.sleep(2)\n",
    "            count_o += 1\n",
    "\n",
    "        with open('../data/review/app_information_dict.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(information_dict, f, indent=4)\n",
    "\n",
    "        print(count_g,count_o)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summary: Sketchfab is your portal to a vast library of 3D models, viewable in 3D, VR, and AR right on your mobile device. Explore historical wonders like Rome, examine intricate anatomical models, or stand in the presence of dinosaurs - all from the palm of your hand.  Interact with models through touch, immerse yourself in VR with a mobile headset, or use AR to bring models into your own environment.  Sketchfab allows you to connect with creators, explore curated collections, and share your favorite discoveries with friends. \\n\\nfeatures: \\n- Explore millions of 3D models\\n- View models in 3D, VR, and AR\\n- Follow favorite creators\\n- Explore by category or keyword search\\n- Like, share, and comment on models\\n- Subscribe to and create collections\\n- Share your profile and discoveries with friends\\n- View curated Staff Picks\\n\\nadjectives: Immersive, Interactive, Educational, Inspiring, Cutting-edge\\n\\nsearch_term: 3D models, VR, AR, 3D design, Augmented Reality \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name     = app_meta['app_name'].iloc[r]\n",
    "descript = app_meta['description'].iloc[r]\n",
    "\n",
    "prompt   = GeminiPrompts(name,descript)\n",
    "response = LLMModel.geminiResponse(prompt,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: Sketchfab is your portal to a vast library of 3D models, viewable in 3D, VR, and AR right on your mobile device. Explore historical wonders like Rome, examine intricate anatomical models, or stand in the presence of dinosaurs - all from the palm of your hand.  Interact with models through touch, immerse yourself in VR with a mobile headset, or use AR to bring models into your own environment.  Sketchfab allows you to connect with creators, explore curated collections, and share your favorite discoveries with friends. \n",
      "\n",
      "features: \n",
      "- Explore millions of 3D models\n",
      "- View models in 3D, VR, and AR\n",
      "- Follow favorite creators\n",
      "- Explore by category or keyword search\n",
      "- Like, share, and comment on models\n",
      "- Subscribe to and create collections\n",
      "- Share your profile and discoveries with friends\n",
      "- View curated Staff Picks\n",
      "\n",
      "adjectives: Immersive, Interactive, Educational, Inspiring, Cutting-edge\n",
      "\n",
      "search_term: 3D models, VR, AR, 3D design, Augmented Reality \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sketchfab is your portal to a vast library of 3D models, viewable in 3D, VR, and AR right on your mobile device. Explore historical wonders like Rome, examine intricate anatomical models, or stand in the presence of dinosaurs - all from the palm of your hand.  Interact with models through touch, immerse yourself in VR with a mobile headset, or use AR to bring models into your own environment.  Sketchfab allows you to connect with creators, explore curated collections, and share your favorite discoveries with friends. \\n\\n', '\\n- Explore millions of 3D models\\n- View models in 3D, VR, and AR\\n- Follow favorite creators\\n- Explore by category or keyword search\\n- Like, share, and comment on models\\n- Subscribe to and create collections\\n- Share your profile and discoveries with friends\\n- View curated Staff Picks\\n\\n', 'Immersive, Interactive, Educational, Inspiring, Cutting-edge\\n\\n', '3D models, VR, AR, 3D design, Augmented Reality \\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extractInformation(text):\n",
    "    fields = [\"\"]*4  # four fields -- summary,features,adjectives,search_term\n",
    " \n",
    "    # finding all the text between summary and features, doing the same for others as well\n",
    "    start_marker = [\"summary: \",\"features: \",\"adjectives: \",\"search_term: \"]\n",
    "    end_marker   = [\"features:\",\"adjectives\",\"search_term\"]\n",
    "    len_prefix   = [9,10,12,13]\n",
    "\n",
    "    for f in range(3):\n",
    "        start_ind = text.index(start_marker[f])\n",
    "        end_ind   = text.index(end_marker[f])\n",
    "        fields[f] = text[start_ind+len_prefix[f]:end_ind]\n",
    "    \n",
    "    start_ind = text.index(start_marker[3])\n",
    "    fields[3] = text[start_ind+len_prefix[-1]:]\n",
    "\n",
    "    return(fields)\n",
    "\n",
    "extracted_data = extractInformation(response)\n",
    "print(extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sketchfab is your portal to a vast library of 3D models, viewable in 3D, VR, and AR right on your mobile device. Explore historical wonders like Rome, examine intricate anatomical models, or stand in the presence of dinosaurs - all from the palm of your hand.  Interact with models through touch, immerse yourself in VR with a mobile headset, or use AR to bring models into your own environment.  Sketchfab allows you to connect with creators, explore curated collections, and share your favorite discoveries with friends. \n",
      "\n",
      "\n",
      "\n",
      "- Explore millions of 3D models\n",
      "- View models in 3D, VR, and AR\n",
      "- Follow favorite creators\n",
      "- Explore by category or keyword search\n",
      "- Like, share, and comment on models\n",
      "- Subscribe to and create collections\n",
      "- Share your profile and discoveries with friends\n",
      "- View curated Staff Picks\n",
      "\n",
      "\n",
      "Immersive, Interactive, Educational, Inspiring, Cutting-edge\n",
      "\n",
      "\n",
      "3D models, VR, AR, 3D design, Augmented Reality \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(extracted_data[0])\n",
    "print(extracted_data[1])\n",
    "print(extracted_data[2])\n",
    "print(extracted_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobilerec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
